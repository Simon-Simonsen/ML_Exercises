{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a168987-8d24-46f9-893a-f99613c96f93",
   "metadata": {},
   "source": [
    "<b>Machine Learning Exercise session: 19th SEPTEMBER 2025</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587de67-4b16-466d-b938-5cf0be968926",
   "metadata": {},
   "source": [
    "# <b>Working with: Bayes Classifier and K-nearest Neighbors</b>\n",
    "\n",
    "Welcome to this week's exercise session. This notebook will take you through: \n",
    "- Understanding Bayes Classifier decision rule\n",
    "- K-nearest Neighbors\n",
    "\n",
    "Remember \n",
    "- It is good practice to use the Machine Learning python environment you made in week 1.\n",
    "- Solving these exercises are supposed to take much longer than 90 minutes. Work on them before going to class.\n",
    "- Learning Machine Learning is challenging. Take your time, make some errors, read documentation if needed. <b>We are happy to help if you are stuck.</b>\n",
    "\n",
    "Have fun! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629a7bc-7f16-4808-a4eb-fbba3570d85e",
   "metadata": {},
   "source": [
    "# <b>Exploring theoretical background</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3918224",
   "metadata": {},
   "source": [
    "### Exercise t1\n",
    "\n",
    "Explain why the Bayes classifier assigns an observation to the class with the highest posterior probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1c3adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06873f4d-1cf8-4761-becc-932ed7c94b37",
   "metadata": {},
   "source": [
    "### Exercise t2\n",
    "\n",
    "Two classes $C_1$ and $C_2$ with feature $x \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "P(x \\mid C_1) = \\mathcal{N}(x \\mid 0, 1), \\quad P(x \\mid C_2) = \\mathcal{N}(x \\mid 2, 1)\n",
    "$$\n",
    "\n",
    "Priors:  \n",
    "\n",
    "$$\n",
    "P(C_1) = 0.5, \\quad P(C_2) = 0.5\n",
    "$$\n",
    "  \n",
    "\n",
    "1. Compute the Bayes classifier decision rule.  \n",
    "2. Find the decision boundary $x^*$. This is the value(s) of $x$ where the Bayes classifier is indifferent between $C_1$ and $C_2$, meaning\n",
    "\n",
    "$$\n",
    "P(C_1 \\mid x) = P(C_2 \\mid x).\n",
    "$$\n",
    "3. Sketch the class-conditional distributions and indicate the decision boundary. Shade the regions classified as $C_1$ and $C_2$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fefaf45f-735d-4556-ae76-d9409097d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54ada4",
   "metadata": {},
   "source": [
    "### Exercise t3\n",
    "\n",
    "In this case the class conditionals for the two classes are the same as previous exercise t2, but this time the priors are:\n",
    "\n",
    "$$\n",
    "P(C_1) = 0.7, \\quad P(C_2) = 0.3\n",
    "$$\n",
    "\n",
    "\n",
    "1. What is the Bayes classifier decision rule? \n",
    "2. Find the new decision boundary $x^*$.  \n",
    "3. Sketch the class-conditional distributions and indicate the new decision boundary.  \n",
    "4. Explain how the prior affects the decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7530a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3adb6f",
   "metadata": {},
   "source": [
    "### Exercise t4\n",
    "\n",
    "Suppose two classes have same class conditional $P(x \\mid C_i)$ but priors\n",
    "$P(C_1)=0.9$ and $P(C_2)=0.1$. What will the Bayes classifier predict most of the time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81fc6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a289b",
   "metadata": {},
   "source": [
    "### Exercise t5\n",
    "\n",
    "a) Why is KNN an approximation of Bayes classifier?\n",
    "\n",
    "b) Discuss how the choice of $K$ affects bias and variance in KNN. \n",
    "\n",
    "c) Consider a dataset with two well-separated classes. What would be the effect of choosing $K=1$ vs $K=n$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd4e6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46ab69-03cf-40ad-856d-faa1291c915a",
   "metadata": {},
   "source": [
    "# <b>Applying what you learned</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195d964",
   "metadata": {},
   "source": [
    "### Documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bac240",
   "metadata": {},
   "source": [
    "- sklearn : https://sklearn.org/stable/getting_started.html\n",
    "\n",
    "- knn: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "- model_selection : https://sklearn.org/stable/api/sklearn.model_selection.html\n",
    "\n",
    "- metrics : https://scikit-learn.org/stable/api/sklearn.metrics.html\n",
    "\n",
    "- preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "740d3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to import more to solve the exercises\n",
    "\n",
    "import numpy as np\n",
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03676dfb",
   "metadata": {},
   "source": [
    "#### The dataset\n",
    "\n",
    "The provided dataset $Wine.csv$ is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. There are three different classes of wine, with respective quantities of 13 constituents found in each of them. \n",
    "\n",
    "You will try to classify the class of wine implementing a K-nearest Neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db13caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obervations:  177\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    13.20        1.78  2.14               11.2        100   \n",
       "1      1    13.16        2.36  2.67               18.6        101   \n",
       "2      1    14.37        1.95  2.50               16.8        113   \n",
       "3      1    13.24        2.59  2.87               21.0        118   \n",
       "4      1    14.20        1.76  2.45               15.2        112   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.65        2.76                  0.26             1.28   \n",
       "1           2.80        3.24                  0.30             2.81   \n",
       "2           3.85        3.49                  0.24             2.18   \n",
       "3           2.80        2.69                  0.39             1.82   \n",
       "4           3.27        3.39                  0.34             1.97   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             4.38  1.05                          3.40     1050  \n",
       "1             5.68  1.03                          3.17     1185  \n",
       "2             7.80  0.86                          3.45     1480  \n",
       "3             4.32  1.04                          2.93      735  \n",
       "4             6.75  1.05                          2.85     1450  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Wine.csv')\n",
    "\n",
    "col_names = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "data.columns = col_names\n",
    "print('Number of obervations: ', len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91753eda-fbc9-4bec-9572-deac0135ce24",
   "metadata": {},
   "source": [
    "### Exercise a1\n",
    "\n",
    "Start by performing an exploratory analysis of the dataset:\n",
    "\n",
    "1) Visualize the distribution of the target classes.\n",
    "\n",
    "2) Inspect which features might be most useful to include in a k-NN classifier (hint: pairplots may help).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aae1317b-b80a-4b2d-b346-0f1baf539fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5511fe6-3e91-4748-af6a-13bb7b47b11d",
   "metadata": {},
   "source": [
    "### Exercise a2\n",
    "\n",
    "Restrict the dataset to only the first two features (Alcohol and Malic acid).\n",
    "\n",
    "Using a 5-nearest neighbors approach, compute the estimated posterior distribution of classes for a new observation ($x_1$=13 ; $x_2$=2.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7140661-6536-4fed-b22a-7703147c5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c846479",
   "metadata": {},
   "source": [
    "### Exercise a3\n",
    "\n",
    "1) Fit a KNN model (k=5) using scikit-learn. Compare its prediction results with those obtained in a2.\n",
    "\n",
    "2) Fit another KNN with k=10.\n",
    "\n",
    "3) Visualize and compare both classifiers by plotting their decision regions in the 2D feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ad3aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba49a2",
   "metadata": {},
   "source": [
    "### Exercise a4\n",
    "\n",
    "1) Perform model selection by finding the optimal value of k, via cross-validation.\n",
    "\n",
    "2) Plot the classification error (or MSE) vs. k, and relate with bias variance trade-off. \n",
    "\n",
    "3) Report the final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d36ba9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a635e31-0f16-427b-8c0b-6d069a4f4eba",
   "metadata": {},
   "source": [
    "# <b>Exploring what you learned</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014c148-1516-4efd-a0c6-63496dc53da4",
   "metadata": {},
   "source": [
    "### Exercise e1\n",
    "\n",
    "You are designing a binary disease screen. Let class $C_2$ be “disease” and $C_1$ be “healthy”. The disease prior is $P(C_2)=0.05$.  \n",
    "\n",
    "1. We make the decision of define a false negative (missed disease) 10 times more costly than a false positive. Write a loss matrix consistent with the statement.  \n",
    "2. Derive the decision rule that minimizes expected posterior loss: the rule that maps $p \\equiv P(C_2 \\mid x)$ to a predicted class.  \n",
    "3. Compute the posterior threshold $p^*$ such that you predict “disease” iff $p \\ge p^*$. Compare this to the standard Bayes threshold under 0–1 loss. Interpret the result in words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d46dac-4a75-41be-b360-a190274385f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c198a0",
   "metadata": {},
   "source": [
    "### Exercise e2\n",
    "\n",
    "We will test the effect of using or not using the loss matrix.\n",
    "\n",
    "1. Generate a synthetic dataset with N examples. Use class conditionals: \n",
    "\n",
    "   $$\n",
    "   x \\mid C_1 \\sim \\mathcal N(0,1), \\qquad\n",
    "   x \\mid C_2 \\sim \\mathcal N(2,1),\n",
    "   $$\n",
    "   with prior $P(C_2)=0.05$. Compute the exact posterior $p(x) = P(C_2 \\mid x)$ using Bayes’ rule with Gaussian densities.\n",
    "\n",
    "2. Implement two classifiers:\n",
    "   - 0–1 loss rule: predict disease if $p(x) \\ge 0.5$.\n",
    "   - Asymmetric-loss rule: use your $p^*$ from previous exercise.\n",
    "\n",
    "   Compare confusion matrices, false negative/positive counts, and expected loss per sample for the two rules. Plot the posterior distribution and show both thresholds.\n",
    "\n",
    "3. Discuss what changed when you used the loss matrix, and whether using the loss matrix had upsides/drawbacks.\n",
    "\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "Confusion matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a53392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
